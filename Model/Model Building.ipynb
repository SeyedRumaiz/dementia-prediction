{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-16T17:47:51.755020Z",
     "start_time": "2025-11-16T17:47:47.294936Z"
    }
   },
   "source": [
    "# ===== IMPORTS =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompose\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ColumnTransformer\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpipeline\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Pipeline\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mimblearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mover_sampling\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SMOTE\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mimblearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpipeline\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Pipeline \u001B[38;5;28;01mas\u001B[39;00m ImbPipeline\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mwarnings\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'imblearn'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:48:10.175052Z",
     "start_time": "2025-11-16T17:48:10.148440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.style.use('default')\n",
    "sns.set_style('white')"
   ],
   "id": "1e2116c4998570ba",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:48:40.045567Z",
     "start_time": "2025-11-16T17:48:29.583571Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('Dementia Prediction Dataset.csv')",
   "id": "94bbd7c65075f17d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/0ks5ncmx5y1c1299d4rhcpgh0000gn/T/ipykernel_50043/1267350436.py:1: DtypeWarning: Columns (20,22,24,26,28,41,44,46,48,51,61,63,65,67,69,71,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,134,156,165,176,179,189,217,220,222,224,226,228,230,232,234,236,238,240,242,244,246,248,250,252,254,256,258,260,262,264,266,268,270,272,382,397,399,401,419,421,423,432,445,454,494,574,605,613,638,674,690,704,707,710,715,727,738,744,746,803,804,809,810,811,812,820,831,833,835,837,843,904,959,960,961,969,970,971,972,982,1004,1007,1010) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Dementia Prediction Dataset.csv')\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:49:00.423231Z",
     "start_time": "2025-11-16T17:49:00.413226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feats = ['AGIT', 'ALCFREQ', 'ALCOHOL', 'ANX', 'ANYMEDS', 'APA', 'APPSEV', 'BEANX',\n",
    "         'BEAPATHY', 'BEDEP', 'BEDISIN', 'BEIRRIT', 'BEOTHR', 'BIRTHYR', 'CBSTROKE',\n",
    "         'DEL', 'DELSEV', 'DEP2YRS', 'DEPD', 'ENERGY', 'FORMVER', 'GAMES', 'HALLSEV',\n",
    "         'HEIGHT', 'INBIRMO', 'INCALLS', 'INCONTF', 'MARISTAT', 'MOFALLS', 'NACCBEHF',\n",
    "         'NACCDAYS', 'NACCLIVS', 'NACCREAS', 'NACCREFR', 'NITE', 'NITESEV', 'PACKSPER',\n",
    "         'RESIDENC', 'SEX', 'SLEEPOTH', 'TAXES']"
   ],
   "id": "1a9f92ad8aed9cb2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:49:11.470295Z",
     "start_time": "2025-11-16T17:49:11.357120Z"
    }
   },
   "cell_type": "code",
   "source": "final_df = pd.concat([df[feats], df['DEMENTED']], axis=1)",
   "id": "6c360f7df35b7737",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:49:17.831559Z",
     "start_time": "2025-11-16T17:49:17.824956Z"
    }
   },
   "cell_type": "code",
   "source": "df = final_df",
   "id": "e133a4dbd2fb7e9b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:50:05.067582Z",
     "start_time": "2025-11-16T17:50:05.036597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Domain knowledge features\n",
    "df['AGE'] = 2025 - df['BIRTHYR']\n",
    "final_df['BEHAV_COMPOSITE'] = final_df['BEANX'] + final_df['BEDEP'] + final_df['BEIRRIT'] + final_df['BEAPATHY']"
   ],
   "id": "fa8b6800439ecc76",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:50:15.914320Z",
     "start_time": "2025-11-16T17:50:15.855143Z"
    }
   },
   "cell_type": "code",
   "source": "df.drop('BIRTHYR', axis=1, inplace=True)",
   "id": "257b2fa0a47f319a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:50:56.178302Z",
     "start_time": "2025-11-16T17:50:56.170078Z"
    }
   },
   "cell_type": "code",
   "source": "feats = [f for f in feats if f != 'BIRTHYR'] + ['AGE', 'BEHAV_COMPOSITE']",
   "id": "9200fe3b0c512616",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:51:13.316434Z",
     "start_time": "2025-11-16T17:51:13.307790Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Total features after feature engineering: {len(feats)}\")",
   "id": "a487612c06feeaf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features after feature engineering: 42\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:52:59.605299Z",
     "start_time": "2025-11-16T17:52:59.510703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===== DATA PREPROCESSING =====\n",
    "print(\"\\n=== DATA PREPROCESSING ===\")\n",
    "\n",
    "X = df.drop('DEMENTED', axis=1)\n",
    "y = df['DEMENTED']"
   ],
   "id": "cbd6395ded1b948b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA PREPROCESSING ===\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:54:45.485479Z",
     "start_time": "2025-11-16T17:54:45.446756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "missing_indicators = []\n",
    "\n",
    "for col in feats:\n",
    "    if X[col].isnull().mean() > 0.05:\n",
    "        name = f\"{col} (missing values)\"\n",
    "        X[name] = X[col].isnull().astype(int)\n",
    "        missing_indicators.append(name)"
   ],
   "id": "3dc1d3d57ab8e23d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T18:09:18.592896Z",
     "start_time": "2025-11-16T18:02:20.442312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===== IMPORTS =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, roc_curve, roc_auc_score, precision_recall_curve\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ===== DATA LOADING =====\n",
    "df = pd.read_csv('Dementia Prediction Dataset.csv')\n",
    "\n",
    "# Original features\n",
    "feats = ['AGIT', 'ALCFREQ', 'ALCOHOL', 'ANX', 'ANYMEDS', 'APA', 'APPSEV', 'BEANX',\n",
    "         'BEAPATHY', 'BEDEP', 'BEDISIN', 'BEIRRIT', 'BEOTHR', 'BIRTHYR', 'CBSTROKE',\n",
    "         'DEL', 'DELSEV', 'DEP2YRS', 'DEPD', 'ENERGY', 'FORMVER', 'GAMES', 'HALLSEV',\n",
    "         'HEIGHT', 'INBIRMO', 'INCALLS', 'INCONTF', 'MARISTAT', 'MOFALLS', 'NACCBEHF',\n",
    "         'NACCDAYS', 'NACCLIVS', 'NACCREAS', 'NACCREFR', 'NITE', 'NITESEV', 'PACKSPER',\n",
    "         'RESIDENC', 'SEX', 'SLEEPOTH', 'TAXES']\n",
    "\n",
    "# Create working dataframe with target\n",
    "final_df = pd.concat([df[feats], df['DEMENTED']], axis=1)\n",
    "\n",
    "# ===== ADVANCED FEATURE ENGINEERING =====\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Create new features from existing numerical data\n",
    "final_df['AGE'] = 2025 - final_df['BIRTHYR']  # Convert birth year to age\n",
    "final_df['BEHAV_COMPOSITE'] = final_df['BEANX'] + final_df['BEDEP'] + final_df['BEIRRIT'] + final_df['BEAPATHY']\n",
    "final_df['COG_FUNCTION_COMPOSITE'] = final_df['FORMVER'] + final_df['GAMES'] + final_df['TAXES']\n",
    "\n",
    "# Drop the original BIRTHYR as we have AGE\n",
    "final_df.drop('BIRTHYR', axis=1, inplace=True)\n",
    "\n",
    "# Update features list\n",
    "feats = [f for f in feats if f != 'BIRTHYR'] + ['AGE', 'BEHAV_COMPOSITE', 'COG_FUNCTION_COMPOSITE']\n",
    "\n",
    "print(f\"Total features after engineering: {len(feats)}\")\n",
    "\n",
    "# ===== DATA PREPROCESSING =====\n",
    "print(\"\\n=== DATA PREPROCESSING ===\")\n",
    "\n",
    "# Separate features and target\n",
    "X = final_df.drop('DEMENTED', axis=1)\n",
    "y = final_df['DEMENTED']\n",
    "\n",
    "print(f\"Original dataset shape: {X.shape}\")\n",
    "\n",
    "# Create missing indicators for columns with >5% missing values\n",
    "missing_indicators = []\n",
    "for col in X.columns:\n",
    "    missing_rate = X[col].isnull().mean()\n",
    "    if missing_rate > 0.05:\n",
    "        indicator_name = f\"{col}_MISSING\"\n",
    "        X[indicator_name] = X[col].isnull().astype(int)\n",
    "        missing_indicators.append(indicator_name)\n",
    "        print(f\"Created missing indicator for {col} (missing rate: {missing_rate:.2%})\")\n",
    "\n",
    "print(f\"Total missing indicators created: {len(missing_indicators)}\")\n",
    "\n",
    "# Handle missing values using KNN Imputation\n",
    "print(\"Applying KNN Imputation for missing values...\")\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_imputed = knn_imputer.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Scale features using RobustScaler (handles outliers better)\n",
    "print(\"Scaling features with RobustScaler...\")\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_imputed.columns)\n",
    "\n",
    "print(f\"Dataset shape after preprocessing: {X_scaled.shape}\")\n",
    "\n",
    "# ===== ADVANCED FEATURE SELECTION =====\n",
    "print(\"\\n=== FEATURE SELECTION ===\")\n",
    "\n",
    "# 1. Remove low variance features\n",
    "print(\"Step 1: Removing low variance features...\")\n",
    "var_threshold = VarianceThreshold(threshold=0.01)\n",
    "X_low_var = var_threshold.fit_transform(X_scaled)\n",
    "selected_features = X_scaled.columns[var_threshold.get_support()].tolist()\n",
    "print(f\"Features after low variance removal: {len(selected_features)}\")\n",
    "\n",
    "# 2. Remove highly correlated features\n",
    "print(\"Step 2: Removing highly correlated features...\")\n",
    "correlation_matrix = pd.DataFrame(X_low_var, columns=selected_features).corr().abs()\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "to_drop = []\n",
    "for column in upper_tri.columns:\n",
    "    high_corr = upper_tri[column][upper_tri[column] > 0.95]\n",
    "    if not high_corr.empty:\n",
    "        to_drop.append(column)\n",
    "\n",
    "if to_drop:\n",
    "    selected_features = [col for col in selected_features if col not in to_drop]\n",
    "    print(f\"Dropped {len(to_drop)} highly correlated features: {to_drop}\")\n",
    "    print(f\"Features after correlation removal: {len(selected_features)}\")\n",
    "\n",
    "# 3. Univariate feature selection\n",
    "print(\"Step 3: Univariate feature selection...\")\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=min(25, len(selected_features)))\n",
    "X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(\n",
    "    X_scaled[selected_features], y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_selected = selector.fit_transform(X_train_temp, y_train_temp)\n",
    "selected_features = np.array(selected_features)[selector.get_support()].tolist()\n",
    "print(f\"Features after univariate selection: {len(selected_features)}\")\n",
    "\n",
    "# ===== TRAIN-TEST SPLIT =====\n",
    "print(\"\\n=== TRAIN-TEST SPLIT ===\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled[selected_features], y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Class distribution - Training: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Class distribution - Test: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# ===== INITIAL RANDOM FOREST FOR FEATURE IMPORTANCE =====\n",
    "print(\"\\n=== INITIAL FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "rf_initial = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_initial.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf_initial.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "# Select top features based on importance (keep at least 15, at most 20)\n",
    "n_features_final = min(20, max(15, len(selected_features)))\n",
    "top_features = feature_importance.head(n_features_final)['feature'].tolist()\n",
    "print(f\"\\nSelected top {len(top_features)} features for final model\")\n",
    "\n",
    "# Update datasets with top features\n",
    "X_train_final = X_train[top_features]\n",
    "X_test_final = X_test[top_features]\n",
    "\n",
    "# ===== COMPREHENSIVE HYPERPARAMETER TUNING =====\n",
    "print(\"\\n=== HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "# Extensive parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [10, 15, 20, 25, 30, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7, 0.9],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', {0: 1, 1: 2}, {0: 1, 1: 3}]\n",
    "}\n",
    "\n",
    "print(\"Starting RandomizedSearchCV with 100 iterations...\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_final, y_train)\n",
    "\n",
    "print(f\"Best score from random search: {random_search.best_score_:.4f}\")\n",
    "print(f\"Best parameters from random search: {random_search.best_params_}\")\n",
    "\n",
    "# Refined grid search around best parameters\n",
    "print(\"\\nStarting refined GridSearchCV...\")\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Create refined grid\n",
    "refined_param_grid = {}\n",
    "for param, value in best_params.items():\n",
    "    if param == 'n_estimators':\n",
    "        refined_param_grid[param] = [max(50, value - 50), value, min(600, value + 50)]\n",
    "    elif param == 'max_depth' and value is not None:\n",
    "        refined_param_grid[param] = [max(5, value - 5), value, value + 5]\n",
    "    elif param == 'min_samples_split':\n",
    "        refined_param_grid[param] = [max(2, value - 2), value, value + 2]\n",
    "    elif param == 'min_samples_leaf':\n",
    "        refined_param_grid[param] = [max(1, value - 1), value, value + 1]\n",
    "    elif param == 'max_features':\n",
    "        refined_param_grid[param] = [value]  # Keep the best value\n",
    "    elif param == 'bootstrap':\n",
    "        refined_param_grid[param] = [value]  # Keep the best value\n",
    "    elif param == 'class_weight':\n",
    "        refined_param_grid[param] = [value]  # Keep the best value\n",
    "\n",
    "# Remove empty entries\n",
    "refined_param_grid = {k: v for k, v in refined_param_grid.items() if v}\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=RandomForestClassifier(random_state=42),\n",
    "#     param_grid=refined_param_grid,\n",
    "#     cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "#     scoring='roc_auc',\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "#\n",
    "# grid_search.fit(X_train_final, y_train)\n",
    "\n",
    "final_model = random_search.best_estimator_\n",
    "# ===== COMPREHENSIVE MODEL EVALUATION =====\n",
    "print(\"\\n=== MODEL EVALUATION ===\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = final_model.predict(X_test_final)\n",
    "y_pred_proba = final_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Dementia', 'Dementia'])\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "plt.title('Confusion Matrix - Optimized Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Dementia', 'Dementia']))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=3, label=f'Random Forest (AUC = {auc_score:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5, label='Random Classifier')\n",
    "plt.fill_between(fpr, tpr, alpha=0.2, color='darkorange')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Dementia Prediction', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "average_precision = np.mean(precision)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall, precision, color='blue', lw=3, label=f'Average Precision = {average_precision:.4f}')\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"upper right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ===== FEATURE IMPORTANCE ANALYSIS =====\n",
    "print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "feature_importance_final = pd.DataFrame({\n",
    "    'feature': top_features,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(feature_importance_final['feature'], feature_importance_final['importance'],\n",
    "         color='skyblue', edgecolor='navy')\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.title('Top Feature Importances - Optimized Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== MODEL EXPLAINABILITY WITH SHAP =====\n",
    "print(\"\\n=== MODEL EXPLAINABILITY ===\")\n",
    "try:\n",
    "    import shap\n",
    "\n",
    "    print(\"Generating SHAP explanations...\")\n",
    "    shap.initjs()\n",
    "\n",
    "    # Create explainer\n",
    "    explainer = shap.TreeExplainer(final_model)\n",
    "    shap_values = explainer.shap_values(X_test_final)\n",
    "\n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values[1], X_test_final, feature_names=top_features, show=False)\n",
    "    plt.title('SHAP Summary Plot - Impact on Dementia Prediction', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Feature importance from SHAP\n",
    "    shap_importance = np.abs(shap_values[1]).mean(0)\n",
    "    shap_feature_importance = pd.DataFrame({\n",
    "        'feature': top_features,\n",
    "        'shap_importance': shap_importance\n",
    "    }).sort_values('shap_importance', ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 features by SHAP importance:\")\n",
    "    print(shap_feature_importance.head(10))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "\n",
    "# ===== FINAL PERFORMANCE SUMMARY =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n",
    "\n",
    "# Save the final model and preprocessing objects\n",
    "print(\"\\n=== SAVING MODEL ===\")\n",
    "joblib.dump(final_model, 'optimized_random_forest_dementia.pkl')\n",
    "joblib.dump(top_features, 'selected_features.pkl')\n",
    "joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "joblib.dump(knn_imputer, 'knn_imputer.pkl')\n",
    "\n",
    "print(\"Model and preprocessing objects saved successfully!\")\n",
    "\n",
    "# ===== COMPARISON WITH BASELINE =====\n",
    "print(\"\\n=== COMPARISON WITH BASELINE ===\")\n",
    "# Simple Random Forest without tuning\n",
    "baseline_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "baseline_rf.fit(X_train_final, y_train)\n",
    "baseline_pred_proba = baseline_rf.predict_proba(X_test_final)[:, 1]\n",
    "baseline_auc = roc_auc_score(y_test, baseline_pred_proba)\n",
    "\n",
    "print(f\"Baseline Random Forest AUC: {baseline_auc:.4f}\")\n",
    "print(f\"Optimized Random Forest AUC: {auc_score:.4f}\")\n",
    "print(f\"Improvement: {auc_score - baseline_auc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ OPTIMIZATION COMPLETE! ðŸŽ¯\")"
   ],
   "id": "9a8fafc3f3d63e83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING ===\n",
      "Total features after engineering: 43\n",
      "\n",
      "=== DATA PREPROCESSING ===\n",
      "Original dataset shape: (195196, 43)\n",
      "Total missing indicators created: 0\n",
      "Applying KNN Imputation for missing values...\n",
      "Scaling features with RobustScaler...\n",
      "Dataset shape after preprocessing: (195196, 43)\n",
      "\n",
      "=== FEATURE SELECTION ===\n",
      "Step 1: Removing low variance features...\n",
      "Features after low variance removal: 43\n",
      "Step 2: Removing highly correlated features...\n",
      "Step 3: Univariate feature selection...\n",
      "Features after univariate selection: 25\n",
      "\n",
      "=== TRAIN-TEST SPLIT ===\n",
      "Training set shape: (156156, 25)\n",
      "Test set shape: (39040, 25)\n",
      "Class distribution - Training: {0: 110084, 1: 46072}\n",
      "Class distribution - Test: {0: 27522, 1: 11518}\n",
      "\n",
      "=== INITIAL FEATURE IMPORTANCE ANALYSIS ===\n",
      "Top 15 most important features:\n",
      "                   feature  importance\n",
      "24  COG_FUNCTION_COMPOSITE    0.222482\n",
      "22                   TAXES    0.204415\n",
      "11                   GAMES    0.129855\n",
      "16                NACCDAYS    0.093503\n",
      "15                NACCBEHF    0.076211\n",
      "4                 BEAPATHY    0.036693\n",
      "2                      APA    0.031494\n",
      "23         BEHAV_COMPOSITE    0.023567\n",
      "19                NACCREFR    0.018884\n",
      "10                  ENERGY    0.018857\n",
      "17                NACCLIVS    0.017608\n",
      "13                 INCALLS    0.015657\n",
      "18                NACCREAS    0.012971\n",
      "0                     AGIT    0.012823\n",
      "20                 NITESEV    0.012678\n",
      "\n",
      "Selected top 20 features for final model\n",
      "\n",
      "=== HYPERPARAMETER TUNING ===\n",
      "Starting RandomizedSearchCV with 100 iterations...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:120: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:120: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:120: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:120: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:120: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:120: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:120: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:120: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  return _ForkingPickler.loads(res)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 191\u001B[39m\n\u001B[32m    179\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting RandomizedSearchCV with 100 iterations...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    180\u001B[39m random_search = RandomizedSearchCV(\n\u001B[32m    181\u001B[39m     estimator=RandomForestClassifier(random_state=\u001B[32m42\u001B[39m),\n\u001B[32m    182\u001B[39m     param_distributions=param_grid,\n\u001B[32m   (...)\u001B[39m\u001B[32m    188\u001B[39m     n_jobs=-\u001B[32m1\u001B[39m\n\u001B[32m    189\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m \u001B[43mrandom_search\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_final\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest score from random search: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrandom_search.best_score_\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    194\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest parameters from random search: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrandom_search.best_params_\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Workspace2/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1358\u001B[39m     estimator._validate_params()\n\u001B[32m   1360\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1361\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1362\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1363\u001B[39m     )\n\u001B[32m   1364\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Workspace2/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1051\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1045\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1046\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1047\u001B[39m     )\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1051\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1054\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1055\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Workspace2/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1992\u001B[39m, in \u001B[36mRandomizedSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1990\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1991\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1992\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1993\u001B[39m \u001B[43m        \u001B[49m\u001B[43mParameterSampler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1994\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_distributions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrandom_state\u001B[49m\n\u001B[32m   1995\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1996\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Workspace2/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:997\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    989\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    990\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    993\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    994\u001B[39m         )\n\u001B[32m    995\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m997\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    998\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    999\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1000\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1001\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1002\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1003\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1005\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1006\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1007\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1008\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m   1016\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1017\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1018\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1019\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1020\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Workspace2/.venv/lib/python3.13/site-packages/sklearn/utils/parallel.py:82\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     73\u001B[39m warning_filters = warnings.filters\n\u001B[32m     74\u001B[39m iterable_with_config_and_warning_filters = (\n\u001B[32m     75\u001B[39m     (\n\u001B[32m     76\u001B[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001B[32m   (...)\u001B[39m\u001B[32m     80\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     81\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config_and_warning_filters\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Workspace2/.venv/lib/python3.13/site-packages/joblib/parallel.py:2072\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2066\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2070\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2072\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Workspace2/.venv/lib/python3.13/site-packages/joblib/parallel.py:1682\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1679\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m   1681\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1684\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1687\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[32m   1688\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Workspace2/.venv/lib/python3.13/site-packages/joblib/parallel.py:1800\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_ordered:\n\u001B[32m   1790\u001B[39m     \u001B[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001B[39;00m\n\u001B[32m   1791\u001B[39m     \u001B[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1795\u001B[39m     \u001B[38;5;66;03m# control only have to be done on the amount of time the next\u001B[39;00m\n\u001B[32m   1796\u001B[39m     \u001B[38;5;66;03m# dispatched job is pending.\u001B[39;00m\n\u001B[32m   1797\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1798\u001B[39m         \u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING\n\u001B[32m   1799\u001B[39m     ):\n\u001B[32m-> \u001B[39m\u001B[32m1800\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1801\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1803\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m nb_jobs == \u001B[32m0\u001B[39m:\n\u001B[32m   1804\u001B[39m     \u001B[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001B[39;00m\n\u001B[32m   1805\u001B[39m     \u001B[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1811\u001B[39m     \u001B[38;5;66;03m# timeouts before any other dispatched job has completed and\u001B[39;00m\n\u001B[32m   1812\u001B[39m     \u001B[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2defa5bccff33d8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
